# ğŸ“š PDF Processing Pipeline

Welcome to the **PDF Processing Pipeline** project! This tool helps you process PDF documents by splitting them into chapter-based chunks, extracting and summarizing text, feeding text to an LLM (Language Learning Model), extracting images, and generating a term weight dependent frequency glossary. Finally, it integrates all this into an interactive dashboard using Streamlit.

## ğŸš€ Features

- ğŸ“„ **PDF Splitting**: Splits PDF into chapter-based chunks.
- ğŸ“ **Text Extraction and Reduction**: Extracts text and reduces it to key points.
- ğŸ¤– **LLM Processing**: Feeds text to a Language Learning Model with defined output parameters.
- ğŸ–¼ï¸ **Image Extraction**: Extracts images from PDF chapters.
- ğŸ“Š **Glossary Generation**: Creates a term weight dependent frequency glossary for each chapter.
- ğŸ–¥ï¸ **Interactive Dashboard**: Allows users to interact with the processed PDF.

## ğŸ› ï¸ Installation

First, clone the repository and navigate to the project directory:

```sh
git clone https://github.com/yourusername/pdf-processing-pipeline.git
cd pdf-processing-pipeline
```

Create and activate a virtual environment (optional but recommended):

```sh
python -m venv env
source env/bin/activate  # On Windows: env\Scripts\activate
```

Install the required dependencies:

```sh
pip install -r requirements.txt
```

## âš™ï¸ Usage

1. **Run the Streamlit App**:

   ```sh
   streamlit run streamlit_app.py
   ```

2. **Upload a PDF**: Choose a PDF file to upload via the Streamlit interface.

3. **Interact with the Dashboard**: View the extracted text, LLM outputs, images, and glossaries.

## ğŸ“ Project Structure

```plaintext
ğŸ“¦ pdf-processing-pipeline
â”œâ”€â”€ ğŸ“‚ data
â”‚   â””â”€â”€ example.pdf                   # Example PDF for testing
â”œâ”€â”€ ğŸ“‚ images                         # Extracted images
â”œâ”€â”€ ğŸ“‚ notebooks                      # Jupyter notebooks for experimentation
â”œâ”€â”€ ğŸ“‚ scripts                        # Python scripts for various processing steps
â”‚   â”œâ”€â”€ extract_text.py               # Text extraction script
â”‚   â”œâ”€â”€ generate_glossary.py          # Glossary generation script
â”‚   â””â”€â”€ process_with_llm.py           # LLM processing script
â”œâ”€â”€ ğŸ“„ requirements.txt               # List of dependencies
â””â”€â”€ ğŸ“„ streamlit_app.py               # Streamlit app for interactive dashboard
```

## ğŸ“œ Detailed Workflow

### 1. Split PDF into Chapters

The PDF is split into chapter-based chunks using simple heuristics to detect chapter titles. This is done using the `fitz` (PyMuPDF) library.

### 2. Concurrent Processing

Chapters are processed concurrently using Pythonâ€™s `concurrent.futures` for efficient processing. Each chapter undergoes:

- **Text Extraction and Reduction**: Extracts text from pages and reduces it.
- **LLM Processing**: Feeds the reduced text to an LLM (e.g., OpenAIâ€™s GPT-3.5) for further processing.
- **Image Extraction**: Extracts images from the chapterâ€™s pages.

### 3. Glossary Generation

A term weight dependent frequency glossary is generated for each chapter using NLP techniques.

### 4. Interactive Dashboard

All the processed data is displayed on a Streamlit dashboard, allowing users to interact with the PDF content holistically.

## ğŸ™Œ Acknowledgments

- [Streamlit](https://www.streamlit.io/) for the amazing dashboarding framework.
- [OpenAI](https://www.openai.com/) for the GPT-3.5 API.
- [PyMuPDF](https://pymupdf.readthedocs.io/) for PDF processing.
- [NLTK](https://www.nltk.org/) for NLP tools.

## ğŸ‘¥ Contributors

- [Rishavs Bhagat](https://github.com/rbrox) - Project Maintainer

Feel free to contribute to this project by submitting issues or pull requests.

## ğŸ“¬ Contact

For any inquiries, please contact [rishavsci@gmail.com](mailto:rishavsci@gmail.com).
